{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSRCT Benchmark - Quick Start Guide\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load confounded datasets\n",
    "2. Access ground truth ATEs\n",
    "3. Evaluate a causal inference method\n",
    "4. Compare multiple methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set paths (adjust if needed)\n",
    "BENCHMARK_DIR = Path('..')  # Parent directory\n",
    "DATASETS_DIR = BENCHMARK_DIR / 'confounded_datasets' / 'by_study'\n",
    "GROUND_TRUTH = BENCHMARK_DIR / 'ground_truth' / 'rct_ates.csv'\n",
    "\n",
    "print(f\"Datasets directory: {DATASETS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a Confounded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific dataset\n",
    "study = 'anchoring1'\n",
    "pattern = 'age'\n",
    "beta = 0.5\n",
    "\n",
    "dataset_path = DATASETS_DIR / study / f'{pattern}_beta{beta}_seed42.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"Dataset: {study}, Pattern: {pattern}, Beta: {beta}\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"\\nColumns: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine treatment distribution\n",
    "print(\"Treatment Distribution:\")\n",
    "print(data['iv'].value_counts())\n",
    "print(f\"\\nTreatment rate: {data['iv'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Ground Truth ATEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "ground_truth = pd.read_csv(GROUND_TRUTH)\n",
    "print(f\"Ground truth for {len(ground_truth)} studies:\\n\")\n",
    "ground_truth[['study', 'n_total', 'ate', 'ate_se']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true ATE for our study\n",
    "true_ate = ground_truth[ground_truth['study'] == study]['ate'].values[0]\n",
    "true_se = ground_truth[ground_truth['study'] == study]['ate_se'].values[0]\n",
    "\n",
    "print(f\"True ATE for {study}: {true_ate:.2f} (SE: {true_se:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Simple Causal Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "def naive_estimator(data, treatment_col='iv', outcome_col='dv'):\n",
    "    \"\"\"Simple difference in means (biased under confounding).\"\"\"\n",
    "    treated = data[data[treatment_col] == 1][outcome_col]\n",
    "    control = data[data[treatment_col] == 0][outcome_col]\n",
    "    \n",
    "    ate = treated.mean() - control.mean()\n",
    "    se = np.sqrt(treated.var()/len(treated) + control.var()/len(control))\n",
    "    \n",
    "    return {\n",
    "        'method': 'naive',\n",
    "        'ate': ate,\n",
    "        'se': se,\n",
    "        'ci_lower': ate - 1.96 * se,\n",
    "        'ci_upper': ate + 1.96 * se\n",
    "    }\n",
    "\n",
    "\n",
    "def ipw_estimator(data, treatment_col='iv', outcome_col='dv', covariates=['resp_age']):\n",
    "    \"\"\"Inverse Probability Weighting estimator.\"\"\"\n",
    "    X = data[covariates].values\n",
    "    T = data[treatment_col].values\n",
    "    Y = data[outcome_col].values\n",
    "    \n",
    "    # Fit propensity score model\n",
    "    ps_model = LogisticRegression(max_iter=1000)\n",
    "    ps_model.fit(X, T)\n",
    "    e = np.clip(ps_model.predict_proba(X)[:, 1], 0.01, 0.99)\n",
    "    \n",
    "    # IPW estimator\n",
    "    weights_1 = T / e\n",
    "    weights_0 = (1 - T) / (1 - e)\n",
    "    \n",
    "    ate_1 = np.sum(Y * weights_1) / np.sum(weights_1)\n",
    "    ate_0 = np.sum(Y * weights_0) / np.sum(weights_0)\n",
    "    ate = ate_1 - ate_0\n",
    "    \n",
    "    # Bootstrap SE (simplified)\n",
    "    se = np.std(Y[T==1])/np.sqrt(np.sum(T)) + np.std(Y[T==0])/np.sqrt(np.sum(1-T))\n",
    "    \n",
    "    return {\n",
    "        'method': 'ipw',\n",
    "        'ate': ate,\n",
    "        'se': se,\n",
    "        'ci_lower': ate - 1.96 * se,\n",
    "        'ci_upper': ate + 1.96 * se\n",
    "    }\n",
    "\n",
    "\n",
    "def outcome_regression(data, treatment_col='iv', outcome_col='dv', covariates=['resp_age']):\n",
    "    \"\"\"Outcome regression estimator.\"\"\"\n",
    "    X = data[covariates].values\n",
    "    T = data[treatment_col].values.reshape(-1, 1)\n",
    "    Y = data[outcome_col].values\n",
    "    \n",
    "    # Fit outcome model\n",
    "    X_full = np.hstack([T, X])\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_full, Y)\n",
    "    \n",
    "    ate = model.coef_[0]  # Coefficient on treatment\n",
    "    \n",
    "    # Simplified SE\n",
    "    residuals = Y - model.predict(X_full)\n",
    "    se = np.std(residuals) / np.sqrt(len(Y))\n",
    "    \n",
    "    return {\n",
    "        'method': 'outcome_regression',\n",
    "        'ate': ate,\n",
    "        'se': se,\n",
    "        'ci_lower': ate - 1.96 * se,\n",
    "        'ci_upper': ate + 1.96 * se\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all methods\n",
    "results = []\n",
    "\n",
    "# Naive\n",
    "naive_result = naive_estimator(data)\n",
    "naive_result['bias'] = naive_result['ate'] - true_ate\n",
    "results.append(naive_result)\n",
    "\n",
    "# IPW\n",
    "ipw_result = ipw_estimator(data, covariates=['resp_age'])\n",
    "ipw_result['bias'] = ipw_result['ate'] - true_ate\n",
    "results.append(ipw_result)\n",
    "\n",
    "# Outcome Regression\n",
    "or_result = outcome_regression(data, covariates=['resp_age'])\n",
    "or_result['bias'] = or_result['ate'] - true_ate\n",
    "results.append(or_result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"True ATE: {true_ate:.2f}\\n\")\n",
    "results_df[['method', 'ate', 'se', 'bias']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "methods = results_df['method'].values\n",
    "ates = results_df['ate'].values\n",
    "errors = results_df['se'].values * 1.96\n",
    "\n",
    "y_pos = np.arange(len(methods))\n",
    "\n",
    "ax.barh(y_pos, ates, xerr=errors, capsize=5, color=['#E74C3C', '#3498DB', '#2ECC71'])\n",
    "ax.axvline(x=true_ate, color='black', linestyle='--', linewidth=2, label=f'True ATE = {true_ate:.0f}')\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([m.replace('_', ' ').title() for m in methods])\n",
    "ax.set_xlabel('Estimated ATE')\n",
    "ax.set_title(f'Method Comparison: {study} (β={beta})')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Across Multiple Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate across different confounding strengths\n",
    "beta_values = [0.1, 0.5, 1.0, 2.0]\n",
    "all_results = []\n",
    "\n",
    "for beta in beta_values:\n",
    "    dataset_path = DATASETS_DIR / study / f'{pattern}_beta{beta}_seed42.csv'\n",
    "    if not dataset_path.exists():\n",
    "        continue\n",
    "        \n",
    "    data = pd.read_csv(dataset_path)\n",
    "    \n",
    "    for method_func, method_name in [\n",
    "        (naive_estimator, 'naive'),\n",
    "        (lambda d: ipw_estimator(d, covariates=['resp_age']), 'ipw'),\n",
    "        (lambda d: outcome_regression(d, covariates=['resp_age']), 'or')\n",
    "    ]:\n",
    "        try:\n",
    "            result = method_func(data)\n",
    "            result['beta'] = beta\n",
    "            result['bias'] = result['ate'] - true_ate\n",
    "            result['abs_bias'] = abs(result['bias'])\n",
    "            all_results.append(result)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "all_results_df = pd.DataFrame(all_results)\n",
    "print(f\"Evaluated {len(all_results_df)} method-dataset combinations\")\n",
    "all_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bias by confounding strength\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "for method in all_results_df['method'].unique():\n",
    "    method_data = all_results_df[all_results_df['method'] == method]\n",
    "    ax.plot(method_data['beta'], method_data['abs_bias'], \n",
    "            marker='o', label=method.replace('_', ' ').title(), linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Confounding Strength (β)')\n",
    "ax.set_ylabel('Absolute Bias')\n",
    "ax.set_title(f'Bias vs Confounding Strength: {study}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Pre-computed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-computed method evaluation results\n",
    "results_dir = BENCHMARK_DIR / 'analysis_results' / 'method_evaluation'\n",
    "\n",
    "if results_dir.exists():\n",
    "    perf_by_method = pd.read_csv(results_dir / 'performance_by_method.csv')\n",
    "    print(\"Pre-computed performance by method:\")\n",
    "    display(perf_by_method)\n",
    "else:\n",
    "    print(\"Pre-computed results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Explore more datasets**: Try different studies, patterns, and beta values\n",
    "2. **Implement your method**: Follow the function signature and return format\n",
    "3. **Submit to leaderboard**: Use the method submission issue template\n",
    "4. **Read documentation**: See README.md for full details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

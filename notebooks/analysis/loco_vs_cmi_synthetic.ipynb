{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: LOCO vs CMI on Synthetic Data\n",
    "\n",
    "This notebook compares the LOCO (Leave-One-Covariate-Out) and CMI (Conditional Mutual Information) \n",
    "methods for conditional independence testing.\n",
    "\n",
    "## Objectives\n",
    "1. Compare Type I error rates (null true scenario)\n",
    "2. Compare statistical power (null false scenarios with varying effect sizes)\n",
    "3. Compare runtime performance\n",
    "4. Evaluate p-value calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from causal_grounding import create_ci_engine, CITestEngine\n",
    "from causal_grounding.ci_tests_loco import LOCOCIEngine\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ci_test_data(\n",
    "    n_samples: int,\n",
    "    n_covariates: int,\n",
    "    effect_size: float,\n",
    "    seed: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic data for CI testing.\n",
    "    \n",
    "    Tests: Y _||_ X | W\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples\n",
    "        n_covariates: Number of conditioning covariates\n",
    "        effect_size: Effect of X on Y (0 = null true, >0 = null false)\n",
    "        seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with X, Y, W1, W2, ..., Wk\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Generate conditioning covariates W\n",
    "    W = {}\n",
    "    for i in range(n_covariates):\n",
    "        W[f'W{i+1}'] = np.random.choice([0, 1, 2], size=n_samples)\n",
    "    \n",
    "    # Generate X (binary)\n",
    "    X = np.random.binomial(1, 0.5, n_samples)\n",
    "    \n",
    "    # Generate Y depending on W and possibly X\n",
    "    logit = -0.5\n",
    "    for i, w_col in enumerate(W.values()):\n",
    "        logit = logit + 0.3 * (w_col == 1) + 0.2 * (w_col == 2)\n",
    "    \n",
    "    # Add X effect (0 for null true, >0 for null false)\n",
    "    logit = logit + effect_size * X\n",
    "    \n",
    "    prob_Y = 1 / (1 + np.exp(-logit))\n",
    "    Y = np.random.binomial(1, prob_Y)\n",
    "    \n",
    "    # Build dataframe\n",
    "    df = pd.DataFrame({'X': X, 'Y': Y})\n",
    "    for name, vals in W.items():\n",
    "        df[name] = vals\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the data generation\n",
    "df_test = generate_ci_test_data(500, 3, effect_size=0, seed=42)\n",
    "print(f\"Generated data shape: {df_test.shape}\")\n",
    "print(f\"Columns: {list(df_test.columns)}\")\n",
    "print(f\"Y mean: {df_test['Y'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "N_REPLICATIONS = 50  # Number of datasets per condition\n",
    "SAMPLE_SIZES = [200, 500, 1000]\n",
    "N_COVARIATES_LIST = [2, 5]\n",
    "EFFECT_SIZES = [0.0, 0.5, 1.0, 1.5]  # 0 = null true\n",
    "ALPHA = 0.05  # Significance level\n",
    "\n",
    "# Create engines\n",
    "cmi_engine = create_ci_engine('cmi', n_permutations=200, random_seed=42)\n",
    "loco_engine = create_ci_engine('loco', function_class='gbm', \n",
    "                                n_estimators=50, max_depth=2, random_state=42)\n",
    "\n",
    "print(f\"Sample sizes: {SAMPLE_SIZES}\")\n",
    "print(f\"Effect sizes: {EFFECT_SIZES}\")\n",
    "print(f\"N replications: {N_REPLICATIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Comparison Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_test(engine, df, conditioning_cols):\n",
    "    \"\"\"Run a single CI test and return results with timing.\"\"\"\n",
    "    start = time.time()\n",
    "    result = engine.test_conditional_independence(df, 'X', 'Y', conditioning_cols)\n",
    "    elapsed = time.time() - start\n",
    "    return {\n",
    "        'p_value': result['p_value'],\n",
    "        'reject': result['reject_independence'],\n",
    "        'cmi': result['cmi'],\n",
    "        'runtime': elapsed\n",
    "    }\n",
    "\n",
    "# Run experiments\n",
    "results = []\n",
    "\n",
    "for n_samples in SAMPLE_SIZES:\n",
    "    for n_cov in N_COVARIATES_LIST:\n",
    "        for effect in EFFECT_SIZES:\n",
    "            print(f\"n={n_samples}, k={n_cov}, effect={effect}\")\n",
    "            \n",
    "            for rep in tqdm(range(N_REPLICATIONS), leave=False):\n",
    "                # Generate data\n",
    "                df = generate_ci_test_data(n_samples, n_cov, effect, seed=rep*1000+n_samples)\n",
    "                conditioning_cols = [f'W{i+1}' for i in range(n_cov)]\n",
    "                \n",
    "                # Run CMI test\n",
    "                try:\n",
    "                    cmi_result = run_single_test(cmi_engine, df, conditioning_cols)\n",
    "                    results.append({\n",
    "                        'method': 'CMI',\n",
    "                        'n_samples': n_samples,\n",
    "                        'n_covariates': n_cov,\n",
    "                        'effect_size': effect,\n",
    "                        'replication': rep,\n",
    "                        **cmi_result\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"CMI error: {e}\")\n",
    "                \n",
    "                # Run LOCO test\n",
    "                try:\n",
    "                    loco_result = run_single_test(loco_engine, df, conditioning_cols)\n",
    "                    results.append({\n",
    "                        'method': 'LOCO',\n",
    "                        'n_samples': n_samples,\n",
    "                        'n_covariates': n_cov,\n",
    "                        'effect_size': effect,\n",
    "                        'replication': rep,\n",
    "                        **loco_result\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"LOCO error: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nTotal results: {len(results_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis: Type I Error (Null True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to null true scenario (effect_size = 0)\n",
    "null_true_df = results_df[results_df['effect_size'] == 0]\n",
    "\n",
    "# Compute rejection rates (Type I error)\n",
    "type1_error = null_true_df.groupby(['method', 'n_samples', 'n_covariates'])['reject'].mean().reset_index()\n",
    "type1_error.columns = ['method', 'n_samples', 'n_covariates', 'type1_error']\n",
    "\n",
    "print(\"Type I Error Rates (should be ~0.05):\")\n",
    "print(type1_error.pivot(index=['n_samples', 'n_covariates'], columns='method', values='type1_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Type I error\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, n_cov in enumerate(N_COVARIATES_LIST):\n",
    "    ax = axes[idx]\n",
    "    subset = type1_error[type1_error['n_covariates'] == n_cov]\n",
    "    \n",
    "    for method in ['CMI', 'LOCO']:\n",
    "        method_data = subset[subset['method'] == method]\n",
    "        ax.plot(method_data['n_samples'], method_data['type1_error'], \n",
    "                marker='o', label=method, linewidth=2)\n",
    "    \n",
    "    ax.axhline(y=0.05, color='red', linestyle='--', label=f'Nominal ({ALPHA})')\n",
    "    ax.set_xlabel('Sample Size')\n",
    "    ax.set_ylabel('Type I Error Rate')\n",
    "    ax.set_title(f'Type I Error (k={n_cov} covariates)')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 0.20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/loco_vs_cmi_type1_error.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis: Statistical Power (Null False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute power (rejection rate when null is false)\n",
    "power_df = results_df.groupby(['method', 'n_samples', 'n_covariates', 'effect_size'])['reject'].mean().reset_index()\n",
    "power_df.columns = ['method', 'n_samples', 'n_covariates', 'effect_size', 'power']\n",
    "\n",
    "print(\"Power by effect size (n=500, k=2):\")\n",
    "subset = power_df[(power_df['n_samples'] == 500) & (power_df['n_covariates'] == 2)]\n",
    "print(subset.pivot(index='effect_size', columns='method', values='power'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot power curves\n",
    "fig, axes = plt.subplots(len(N_COVARIATES_LIST), len(SAMPLE_SIZES), \n",
    "                         figsize=(4*len(SAMPLE_SIZES), 4*len(N_COVARIATES_LIST)))\n",
    "\n",
    "for i, n_cov in enumerate(N_COVARIATES_LIST):\n",
    "    for j, n_samples in enumerate(SAMPLE_SIZES):\n",
    "        ax = axes[i, j] if len(N_COVARIATES_LIST) > 1 else axes[j]\n",
    "        subset = power_df[(power_df['n_samples'] == n_samples) & \n",
    "                          (power_df['n_covariates'] == n_cov)]\n",
    "        \n",
    "        for method in ['CMI', 'LOCO']:\n",
    "            method_data = subset[subset['method'] == method]\n",
    "            ax.plot(method_data['effect_size'], method_data['power'], \n",
    "                    marker='o', label=method, linewidth=2)\n",
    "        \n",
    "        ax.axhline(y=0.05, color='red', linestyle='--', alpha=0.5)\n",
    "        ax.axhline(y=0.80, color='green', linestyle='--', alpha=0.5, label='80% power')\n",
    "        ax.set_xlabel('Effect Size')\n",
    "        ax.set_ylabel('Power')\n",
    "        ax.set_title(f'n={n_samples}, k={n_cov}')\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/loco_vs_cmi_power_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analysis: Runtime Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average runtime by method and sample size\n",
    "runtime_df = results_df.groupby(['method', 'n_samples', 'n_covariates'])['runtime'].agg(['mean', 'std']).reset_index()\n",
    "runtime_df.columns = ['method', 'n_samples', 'n_covariates', 'mean_runtime', 'std_runtime']\n",
    "\n",
    "print(\"Mean Runtime (seconds):\")\n",
    "print(runtime_df.pivot(index=['n_samples', 'n_covariates'], columns='method', values='mean_runtime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot runtime comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, n_cov in enumerate(N_COVARIATES_LIST):\n",
    "    ax = axes[idx]\n",
    "    subset = runtime_df[runtime_df['n_covariates'] == n_cov]\n",
    "    \n",
    "    x = np.arange(len(SAMPLE_SIZES))\n",
    "    width = 0.35\n",
    "    \n",
    "    cmi_data = subset[subset['method'] == 'CMI']\n",
    "    loco_data = subset[subset['method'] == 'LOCO']\n",
    "    \n",
    "    ax.bar(x - width/2, cmi_data['mean_runtime'], width, label='CMI', \n",
    "           yerr=cmi_data['std_runtime'], capsize=5)\n",
    "    ax.bar(x + width/2, loco_data['mean_runtime'], width, label='LOCO',\n",
    "           yerr=loco_data['std_runtime'], capsize=5)\n",
    "    \n",
    "    ax.set_xlabel('Sample Size')\n",
    "    ax.set_ylabel('Runtime (seconds)')\n",
    "    ax.set_title(f'Runtime Comparison (k={n_cov} covariates)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(SAMPLE_SIZES)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/loco_vs_cmi_runtime.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis: P-value Calibration (Under Null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-value distribution under null (should be uniform)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, method in enumerate(['CMI', 'LOCO']):\n",
    "    ax = axes[idx]\n",
    "    method_null = null_true_df[null_true_df['method'] == method]\n",
    "    \n",
    "    ax.hist(method_null['p_value'], bins=20, density=True, alpha=0.7, edgecolor='black')\n",
    "    ax.axhline(y=1.0, color='red', linestyle='--', label='Uniform')\n",
    "    ax.set_xlabel('P-value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{method} P-value Distribution (Under Null)')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/loco_vs_cmi_pvalue_calibration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov test for uniformity\n",
    "print(\"KS Test for P-value Uniformity (under null):\")\n",
    "for method in ['CMI', 'LOCO']:\n",
    "    pvals = null_true_df[null_true_df['method'] == method]['p_value']\n",
    "    ks_stat, ks_pval = stats.kstest(pvals, 'uniform')\n",
    "    print(f\"  {method}: KS stat = {ks_stat:.4f}, p-value = {ks_pval:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary = results_df.groupby(['method', 'effect_size']).agg({\n",
    "    'reject': 'mean',\n",
    "    'runtime': 'mean',\n",
    "    'p_value': 'mean'\n",
    "}).round(4)\n",
    "summary.columns = ['Rejection Rate', 'Mean Runtime (s)', 'Mean P-value']\n",
    "\n",
    "print(\"Summary by Method and Effect Size:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv('../results/loco_vs_cmi_synthetic_results.csv', index=False)\n",
    "print(\"Results saved to results/loco_vs_cmi_synthetic_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "Based on the experiments:\n",
    "\n",
    "1. **Type I Error**: [Fill in based on results]\n",
    "2. **Power**: [Fill in based on results]\n",
    "3. **Runtime**: [Fill in based on results]\n",
    "4. **P-value Calibration**: [Fill in based on results]\n",
    "\n",
    "**Recommendations**:\n",
    "- Use CMI when: [conditions]\n",
    "- Use LOCO when: [conditions]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: LOCO vs CMI on OSRCT Data\n",
    "\n",
    "This notebook compares LOCO and CMI methods for instrument ranking on real OSRCT data.\n",
    "\n",
    "## Objectives\n",
    "1. Compare EHS scores across methods\n",
    "2. Evaluate rank correlation (Spearman)\n",
    "3. Check top-k instrument agreement\n",
    "4. Compare runtime on real data\n",
    "5. Analyze downstream impact on bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from causal_grounding import (\n",
    "    create_ci_engine,\n",
    "    rank_covariates,\n",
    "    rank_covariates_across_sites,\n",
    "    select_best_instrument,\n",
    "    create_train_target_split,\n",
    "    load_rct_data,\n",
    "    discretize_covariates\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load OSRCT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to data\n",
    "osrct_path = Path('../confounded_datasets/anchoring1/anchoring1_age_beta_0.3.pkl')\n",
    "rct_path = Path('../ManyLabs1/ML1_data_cleaned_discretized.pkl')\n",
    "\n",
    "print(f\"OSRCT path exists: {osrct_path.exists()}\")\n",
    "print(f\"RCT path exists: {rct_path.exists()}\")\n",
    "\n",
    "if osrct_path.exists() and rct_path.exists():\n",
    "    osrct_data = pd.read_pickle(osrct_path)\n",
    "    rct_data = load_rct_data('anchoring1', str(rct_path))\n",
    "    \n",
    "    print(f\"\\nOSRCT shape: {osrct_data.shape}\")\n",
    "    print(f\"RCT shape: {rct_data.shape}\")\n",
    "    print(f\"\\nColumns: {list(osrct_data.columns)}\")\n",
    "else:\n",
    "    print(\"Data files not found. Please check the paths.\")\n",
    "    print(\"You may need to run the data preprocessing scripts first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/target split\n",
    "target_site = 'mturk'\n",
    "\n",
    "if 'osrct_data' in dir() and 'rct_data' in dir():\n",
    "    training_data, target_data = create_train_target_split(\n",
    "        osrct_data, rct_data, target_site=target_site\n",
    "    )\n",
    "    \n",
    "    print(f\"Training sites: {list(training_data.keys())}\")\n",
    "    print(f\"Target site: {target_site}\")\n",
    "    print(f\"\\nSamples per training site:\")\n",
    "    for site, df in training_data.items():\n",
    "        print(f\"  {site}: {len(df)} (idle: {(df['F']=='idle').sum()}, on: {(df['F']=='on').sum()})\")\n",
    "    print(f\"\\nTarget samples: {len(target_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define treatment, outcome, and covariates\n",
    "treatment = 'iv'  # Intervention variable\n",
    "outcome = 'dv'    # Dependent variable\n",
    "\n",
    "# Covariates to test as instruments\n",
    "covariates = ['age_d', 'polideo_d', 'gender']  # Discretized covariates\n",
    "\n",
    "# Check if all covariates exist\n",
    "if 'training_data' in dir():\n",
    "    sample_site = list(training_data.keys())[0]\n",
    "    sample_df = training_data[sample_site]\n",
    "    available_cols = sample_df.columns.tolist()\n",
    "    \n",
    "    print(f\"Treatment: {treatment} (exists: {treatment in available_cols})\")\n",
    "    print(f\"Outcome: {outcome} (exists: {outcome in available_cols})\")\n",
    "    print(f\"\\nCovariates:\")\n",
    "    for cov in covariates:\n",
    "        exists = cov in available_cols\n",
    "        print(f\"  {cov}: exists={exists}\")\n",
    "        if exists:\n",
    "            print(f\"    Values: {sample_df[cov].unique()[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create CI Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engines for comparison\n",
    "cmi_engine = create_ci_engine('cmi', n_permutations=500, random_seed=42)\n",
    "loco_engine = create_ci_engine('loco', function_class='gbm', \n",
    "                                n_estimators=100, max_depth=3, random_state=42)\n",
    "\n",
    "print(\"Engines created:\")\n",
    "print(f\"  CMI: {type(cmi_engine).__name__}\")\n",
    "print(f\"  LOCO: {type(loco_engine).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run EHS Scoring with Both Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_site_with_engine(site_data, engine, covariates, treatment, outcome):\n",
    "    \"\"\"Score all covariates for a single site with timing.\"\"\"\n",
    "    # Filter to idle regime\n",
    "    idle_data = site_data[site_data['F'] == 'idle'].copy()\n",
    "    \n",
    "    results = []\n",
    "    start = time.time()\n",
    "    \n",
    "    for z_a in covariates:\n",
    "        z_b = [z for z in covariates if z != z_a]\n",
    "        try:\n",
    "            score_result = engine.score_ehs_criteria(\n",
    "                idle_data, z_a, z_b, treatment, outcome\n",
    "            )\n",
    "            results.append(score_result)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error scoring {z_a}: {e}\")\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    return pd.DataFrame(results), elapsed\n",
    "\n",
    "# Run scoring for each site\n",
    "if 'training_data' in dir():\n",
    "    cmi_results = {}\n",
    "    loco_results = {}\n",
    "    cmi_times = {}\n",
    "    loco_times = {}\n",
    "    \n",
    "    for site, site_data in training_data.items():\n",
    "        print(f\"\\nProcessing site: {site}\")\n",
    "        \n",
    "        # CMI scoring\n",
    "        print(\"  Running CMI...\")\n",
    "        cmi_df, cmi_time = score_site_with_engine(\n",
    "            site_data, cmi_engine, covariates, treatment, outcome\n",
    "        )\n",
    "        cmi_results[site] = cmi_df\n",
    "        cmi_times[site] = cmi_time\n",
    "        print(f\"    CMI time: {cmi_time:.2f}s\")\n",
    "        \n",
    "        # LOCO scoring\n",
    "        print(\"  Running LOCO...\")\n",
    "        loco_df, loco_time = score_site_with_engine(\n",
    "            site_data, loco_engine, covariates, treatment, outcome\n",
    "        )\n",
    "        loco_results[site] = loco_df\n",
    "        loco_times[site] = loco_time\n",
    "        print(f\"    LOCO time: {loco_time:.2f}s\")\n",
    "    \n",
    "    print(\"\\n=== Scoring Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rankings(cmi_df, loco_df, site_name):\n",
    "    \"\"\"Compare rankings from CMI and LOCO.\"\"\"\n",
    "    # Sort by score\n",
    "    cmi_ranked = cmi_df.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    loco_ranked = loco_df.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Create comparison\n",
    "    comparison = pd.DataFrame({\n",
    "        'z_a': cmi_ranked['z_a'],\n",
    "        'cmi_rank': range(1, len(cmi_ranked)+1),\n",
    "        'cmi_score': cmi_ranked['score'],\n",
    "        'cmi_passes_ehs': cmi_ranked['passes_ehs'],\n",
    "    })\n",
    "    \n",
    "    # Add LOCO results\n",
    "    loco_rank_map = {row['z_a']: i+1 for i, row in loco_ranked.iterrows()}\n",
    "    loco_score_map = {row['z_a']: row['score'] for _, row in loco_ranked.iterrows()}\n",
    "    loco_ehs_map = {row['z_a']: row['passes_ehs'] for _, row in loco_ranked.iterrows()}\n",
    "    \n",
    "    comparison['loco_rank'] = comparison['z_a'].map(loco_rank_map)\n",
    "    comparison['loco_score'] = comparison['z_a'].map(loco_score_map)\n",
    "    comparison['loco_passes_ehs'] = comparison['z_a'].map(loco_ehs_map)\n",
    "    \n",
    "    # Compute Spearman correlation\n",
    "    spearman_corr, spearman_p = stats.spearmanr(\n",
    "        comparison['cmi_rank'], comparison['loco_rank']\n",
    "    )\n",
    "    \n",
    "    return comparison, spearman_corr, spearman_p\n",
    "\n",
    "# Compare rankings for each site\n",
    "if 'cmi_results' in dir() and 'loco_results' in dir():\n",
    "    comparisons = {}\n",
    "    correlations = {}\n",
    "    \n",
    "    for site in cmi_results.keys():\n",
    "        comp, rho, pval = compare_rankings(cmi_results[site], loco_results[site], site)\n",
    "        comparisons[site] = comp\n",
    "        correlations[site] = {'spearman_rho': rho, 'p_value': pval}\n",
    "        \n",
    "        print(f\"\\n=== Site: {site} ===\")\n",
    "        print(comp.to_string(index=False))\n",
    "        print(f\"\\nSpearman correlation: rho={rho:.3f}, p={pval:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparisons' in dir() and len(comparisons) > 0:\n",
    "    n_sites = len(comparisons)\n",
    "    fig, axes = plt.subplots(1, n_sites, figsize=(5*n_sites, 5))\n",
    "    if n_sites == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (site, comp) in enumerate(comparisons.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Scatter plot of scores\n",
    "        ax.scatter(comp['cmi_score'], comp['loco_score'], s=100)\n",
    "        \n",
    "        # Add labels\n",
    "        for _, row in comp.iterrows():\n",
    "            ax.annotate(row['z_a'], \n",
    "                       (row['cmi_score'], row['loco_score']),\n",
    "                       textcoords=\"offset points\",\n",
    "                       xytext=(5, 5))\n",
    "        \n",
    "        # Add diagonal line\n",
    "        min_val = min(comp['cmi_score'].min(), comp['loco_score'].min())\n",
    "        max_val = max(comp['cmi_score'].max(), comp['loco_score'].max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel('CMI Score')\n",
    "        ax.set_ylabel('LOCO Score')\n",
    "        rho = correlations[site]['spearman_rho']\n",
    "        ax.set_title(f'{site} (rho={rho:.2f})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/loco_vs_cmi_osrct_scores.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Runtime Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cmi_times' in dir() and 'loco_times' in dir():\n",
    "    runtime_comparison = pd.DataFrame({\n",
    "        'site': list(cmi_times.keys()),\n",
    "        'cmi_time': list(cmi_times.values()),\n",
    "        'loco_time': list(loco_times.values())\n",
    "    })\n",
    "    runtime_comparison['speedup'] = runtime_comparison['cmi_time'] / runtime_comparison['loco_time']\n",
    "    \n",
    "    print(\"Runtime Comparison (seconds):\")\n",
    "    print(runtime_comparison.to_string(index=False))\n",
    "    print(f\"\\nMean CMI time: {runtime_comparison['cmi_time'].mean():.2f}s\")\n",
    "    print(f\"Mean LOCO time: {runtime_comparison['loco_time'].mean():.2f}s\")\n",
    "    print(f\"Mean speedup (CMI/LOCO): {runtime_comparison['speedup'].mean():.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot runtime comparison\n",
    "if 'runtime_comparison' in dir():\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    x = np.arange(len(runtime_comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, runtime_comparison['cmi_time'], width, label='CMI')\n",
    "    ax.bar(x + width/2, runtime_comparison['loco_time'], width, label='LOCO')\n",
    "    \n",
    "    ax.set_xlabel('Site')\n",
    "    ax.set_ylabel('Runtime (seconds)')\n",
    "    ax.set_title('Runtime Comparison: CMI vs LOCO')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(runtime_comparison['site'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/loco_vs_cmi_osrct_runtime.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Top Instrument Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparisons' in dir():\n",
    "    print(\"Top Instrument Selection:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    agreement_count = 0\n",
    "    total_sites = len(comparisons)\n",
    "    \n",
    "    for site, comp in comparisons.items():\n",
    "        cmi_top = comp.loc[comp['cmi_rank'] == 1, 'z_a'].values[0]\n",
    "        loco_top = comp.loc[comp['loco_rank'] == 1, 'z_a'].values[0]\n",
    "        \n",
    "        agrees = cmi_top == loco_top\n",
    "        if agrees:\n",
    "            agreement_count += 1\n",
    "        \n",
    "        print(f\"{site}:\")\n",
    "        print(f\"  CMI top: {cmi_top}\")\n",
    "        print(f\"  LOCO top: {loco_top}\")\n",
    "        print(f\"  Agreement: {'YES' if agrees else 'NO'}\")\n",
    "    \n",
    "    print(f\"\\nOverall Agreement: {agreement_count}/{total_sites} sites ({100*agreement_count/total_sites:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. EHS Criteria Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comparisons' in dir():\n",
    "    print(\"EHS Criteria Agreement:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    ehs_agreement = []\n",
    "    \n",
    "    for site, comp in comparisons.items():\n",
    "        for _, row in comp.iterrows():\n",
    "            agrees = row['cmi_passes_ehs'] == row['loco_passes_ehs']\n",
    "            ehs_agreement.append({\n",
    "                'site': site,\n",
    "                'z_a': row['z_a'],\n",
    "                'cmi_passes': row['cmi_passes_ehs'],\n",
    "                'loco_passes': row['loco_passes_ehs'],\n",
    "                'agrees': agrees\n",
    "            })\n",
    "    \n",
    "    ehs_df = pd.DataFrame(ehs_agreement)\n",
    "    print(ehs_df.to_string(index=False))\n",
    "    print(f\"\\nOverall EHS Agreement: {ehs_df['agrees'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'correlations' in dir() and 'runtime_comparison' in dir():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY: LOCO vs CMI on OSRCT Data\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Rank correlation\n",
    "    mean_rho = np.mean([c['spearman_rho'] for c in correlations.values()])\n",
    "    print(f\"\\n1. Rank Correlation (Spearman):\")\n",
    "    print(f\"   Mean rho = {mean_rho:.3f}\")\n",
    "    if mean_rho > 0.7:\n",
    "        print(\"   -> Strong agreement between methods\")\n",
    "    elif mean_rho > 0.4:\n",
    "        print(\"   -> Moderate agreement between methods\")\n",
    "    else:\n",
    "        print(\"   -> Weak agreement - methods may produce different rankings\")\n",
    "    \n",
    "    # Runtime\n",
    "    mean_speedup = runtime_comparison['speedup'].mean()\n",
    "    print(f\"\\n2. Runtime:\")\n",
    "    print(f\"   Mean CMI time: {runtime_comparison['cmi_time'].mean():.2f}s\")\n",
    "    print(f\"   Mean LOCO time: {runtime_comparison['loco_time'].mean():.2f}s\")\n",
    "    if mean_speedup > 1:\n",
    "        print(f\"   -> CMI is {mean_speedup:.1f}x slower than LOCO\")\n",
    "    else:\n",
    "        print(f\"   -> LOCO is {1/mean_speedup:.1f}x slower than CMI\")\n",
    "    \n",
    "    # EHS Agreement\n",
    "    if 'ehs_df' in dir():\n",
    "        ehs_agree_rate = ehs_df['agrees'].mean()\n",
    "        print(f\"\\n3. EHS Criteria Agreement: {ehs_agree_rate*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nUse CMI when:\")\n",
    "    print(\"  - Data is discrete/categorical\")\n",
    "    print(\"  - Information-theoretic interpretation needed\")\n",
    "    print(\"\\nUse LOCO when:\")\n",
    "    print(\"  - Data has continuous covariates\")\n",
    "    print(\"  - High-dimensional conditioning sets\")\n",
    "    print(\"  - Predictive interpretation preferred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "if 'comparisons' in dir():\n",
    "    all_comparisons = pd.concat([df.assign(site=site) for site, df in comparisons.items()])\n",
    "    all_comparisons.to_csv('../results/loco_vs_cmi_osrct_rankings.csv', index=False)\n",
    "    \n",
    "    correlations_df = pd.DataFrame([\n",
    "        {'site': site, **corr} for site, corr in correlations.items()\n",
    "    ])\n",
    "    correlations_df.to_csv('../results/loco_vs_cmi_osrct_correlations.csv', index=False)\n",
    "    \n",
    "    if 'runtime_comparison' in dir():\n",
    "        runtime_comparison.to_csv('../results/loco_vs_cmi_osrct_runtime.csv', index=False)\n",
    "    \n",
    "    print(\"Results saved to results/ directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da884587",
   "metadata": {},
   "source": [
    "-----\n",
    "# Causal Discovery Grounding: an Implementation\n",
    "\n",
    "Code for the basic method with a basic simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5377c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library loading\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from simulator import *\n",
    "\n",
    "from scipy.optimize import linprog\n",
    "from tqdm import tqdm\n",
    "\n",
    "synthetic_demo_1 = False\n",
    "debug_bounds = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf2316",
   "metadata": {},
   "source": [
    "--------\n",
    "## Placeholder Background Tools\n",
    "\n",
    "The following are simple implementations of conditional independence and simulators that should be substituted with better alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e93a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional independence test\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def regression_indep(df, x_idx, a_idx, y_idx):\n",
    "    \n",
    "  \"\"\"\n",
    "  Finds independence between covariates and output using sparse logistic regression.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "\n",
    "  df    : pandas DataFrame\n",
    "  x_idx : list of strings with the column names corresponding to the covariates \n",
    "  a_idx : string with the name of the (binary) treatment variable\n",
    "  y_idx : string with the name of the (binary) outcome variable\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "    \n",
    "  drop_Y : list of binary variables, drop_Y[i] == 1 indicates that variable\n",
    "           correspoding to x_idx[i] has been dropped in the regression of outcome\n",
    "           on covariates X and treament A\n",
    "  drop_X : similar, but the regression is of the treatment A on covariantes X only\n",
    "  \"\"\"\n",
    "\n",
    "  def get_indep_sel(model_reg, names_in):\n",
    "      \n",
    "    ohe = model_reg.named_steps[\"onehot\"]\n",
    "    feature_names = ohe.get_feature_names_out(names_in)\n",
    "    coef = model_reg.named_steps[\"logreg\"].coef_[0]\n",
    "\n",
    "    indep_list = np.zeros(len(x_idx), dtype=np.int_)\n",
    "    for j, x_name in enumerate(x_idx):\n",
    "      indices = [i for i, s in enumerate(feature_names) if s.startswith(x_name)]\n",
    "    if sum(coef[indices]==0) >= 0.5 * len(indices):\n",
    "      indep_list[j] = 1\n",
    "\n",
    "    return indep_list\n",
    "    \n",
    "  xa_idx = x_idx.copy()\n",
    "  xa_idx.extend([a_idx])\n",
    "  A = df[a_idx]\n",
    "  Y = df[y_idx]\n",
    "  X = df[x_idx]\n",
    "  XA = df[xa_idx]\n",
    "\n",
    "  model_reg = Pipeline([\n",
    "      (\"onehot\", OneHotEncoder(drop=\"first\", sparse_output=True)),\n",
    "      (\"logreg\", LogisticRegression(\n",
    "          penalty=\"l1\",\n",
    "          solver=\"saga\",\n",
    "          max_iter=5000,\n",
    "          C=0.1       # smaller C → more sparsity\n",
    "      ))\n",
    "  ])\n",
    "\n",
    "  model_reg.fit(XA, Y)\n",
    "  drop_Y = get_indep_sel(model_reg, xa_idx)\n",
    "  model_reg.fit(X, A)\n",
    "  drop_A = get_indep_sel(model_reg, x_idx)\n",
    "\n",
    "  return drop_Y, drop_A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02139337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and data simulation\n",
    "\n",
    "def simulate_plain_data_collection(k, n_obs, n_rct, num_x, p_sparse):\n",
    "\n",
    "  \"\"\"\n",
    "  The plainest simulator: generate a bunch of binary models with independent covariates,\n",
    "  with corresponding observational and RCT samples. The \"prior\" tying them together is\n",
    "  a very broad one (the simulator inside `simulate_binary_synthetic_backdoor_parameters`), \n",
    "  so problems are not well-connected. True sparsity is imposed on the outcome model.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "\n",
    "  k        : number of datasets\n",
    "  n_obs    : sample size for observational datasets\n",
    "  n_rct    : sample size for RCTs\n",
    "  num_x    : number of covariates\n",
    "  p_sparse : probability of dropping a covariate into the outcome model\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "    \n",
    "  true_models : models of the type `BinarySyntheticBackdoorModel`\n",
    "  dat_obs     : a Pandas data frame with the corresponding observational data\n",
    "  dat_rct     : a Pandas data frame with the corresponding randomized trial\n",
    "  true_cates  : true CATEs evaluated at the corresponding dat_obs datasets\n",
    "  \"\"\"\n",
    "\n",
    "  sparse_pattern = np.random.binomial(n=1, p=p_sparse, size=num_x)\n",
    "\n",
    "  true_models = [None] * k\n",
    "  dat_obs = [None] * k\n",
    "  dat_rct = [None] * k\n",
    "  true_cates = [None] * k\n",
    "\n",
    "  for i in range(k):\n",
    "    true_models[i] = simulate_binary_synthetic_backdoor_parameters(num_x, sparse_pattern)\n",
    "    dat_obs[i] = simulate_from_backdoor_model(n_obs, true_models[i])\n",
    "    dat_rct[i] = simulate_from_controlled_backdoor_model(n_rct, true_models[i])\n",
    "    true_cates[i] = get_cate(dat_obs[i], true_models[i])\n",
    "\n",
    "  return true_models, dat_obs, dat_rct, true_cates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97b7a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ILLUSTRATIVE DATA SIMULATION\n",
    "#\n",
    "# Purely synthetic case where all variables are binary and \n",
    "\n",
    "\n",
    "if synthetic_demo_1:\n",
    "\n",
    "  n = 1000       # Sample size to simulate a fitting procedure\n",
    "  num_x = 20     # Number of covariates\n",
    "  p_sparse = 0.2 # Probability of dropping a covariate from the outcome model\n",
    "  sparse_pattern = np.random.binomial(n=1, p=p_sparse, size=num_x)\n",
    "\n",
    "  # Generate synthetic data from fitted model, along with ground truth CATE\n",
    "  print('Simulating ground truth model...')\n",
    "  true_model = simulate_binary_synthetic_backdoor_parameters(num_x, sparse_pattern)\n",
    "  model_sample = simulate_from_backdoor_model(n, true_model)\n",
    "  true_cate = get_cate(model_sample, true_model)\n",
    "\n",
    "  # Fit model with all covariates\n",
    "  print('Fitting causal effect estimators...')\n",
    "  cate_hat_full, _ = estimate_cate_tlearner(model_sample, true_model.x_idx, true_model)\n",
    "\n",
    "  print()\n",
    "\n",
    "  # Please notice that the CATE cases are not directly comparable, as the CATE changes between problems\n",
    "  print('RESULTS FOR SYNTHETIC DATA EXPERIMENT')\n",
    "  print('------------------------------------------')\n",
    "  print('True CATE standard deviation:', np.mean(abs(true_cate - cate_hat_full)))\n",
    "  print('Mean absolute CATE error:', np.mean(abs(true_cate - cate_hat_full)))\n",
    "  print('ATE error:', np.mean(true_cate) - np.mean(cate_hat_full))\n",
    "  print('CATE rank correlation:', spearmanr(true_cate, cate_hat_full).statistic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38d6a5f",
   "metadata": {},
   "source": [
    "----\n",
    "## Main Algorithm\n",
    "\n",
    "The following are the core functions for the discovery grounding algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c28588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for bounding inference\n",
    "\n",
    "def search_adjustment(dat, x_idx, a_idx, y_idx):\n",
    "\n",
    "  \"\"\"\n",
    "  Search for possible pseudo-intervention variable (\"instruments\") that can be found\n",
    "  in observational data `dat`. This is done by running a conditional independence criteria\n",
    "  that tests for independence of outcome and possible instrument Z_A given all other covariates\n",
    "  and treatment variable, as well a conditional independence assessment of Z_A and treatment\n",
    "  given all other covariates. If independence is detected in the first case but not the latter one,\n",
    "  then Z_A is selected as a instrumental variable.\n",
    "  \n",
    "  Parameters:\n",
    "  -----------\n",
    "\n",
    "  dat   : observational data, a Pandas dataframe\n",
    "  x_idx : list of strings with the column names corresponding to the covariates \n",
    "  a_idx : string with the name of the (binary) treatment variable\n",
    "  y_idx : string with the name of the (binary) outcome variable\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "\n",
    "  instruments: a binary array indicating which variables passed the test,\n",
    "                        where the positions correspond to positions in `x_idx`\n",
    "  \"\"\"\n",
    "\n",
    "  drop_Y, drop_A = regression_indep(dat, x_idx, a_idx, y_idx)\n",
    "  drop_in = (drop_Y == 1) * (drop_A == 0)\n",
    "  instruments = [i for i, value in enumerate(drop_in) if value == 1]\n",
    "\n",
    "  return instruments\n",
    "\n",
    "def get_predictive_model(train_obs, train_rct, x_idx, a_idx, y_idx, instruments):\n",
    "  \n",
    "  \"\"\"\n",
    "  Fits models for predicting treatment, and outcome under fixed treatments.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "\n",
    "  train_obs : pandas DataFrame collected under observational conditions\n",
    "  train_rct : pandas DataFrame collected under controlled conditions\n",
    "  x_idx     : list of strings with the column names corresponding to the covariates \n",
    "  a_idx     : string with the name of the (binary) treatment variable\n",
    "  y_idx     : string with the name of the (binary) outcome variable\n",
    "  instruments: subset of covariates (given as positions in `x_idx`) which\n",
    "               should not be used for the outcome model\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "    \n",
    "  model_a  : XGBoost model that predicts P(A = 1 | X = x)\n",
    "  model_y1 : XGBoost model that predicts P(Y = 1 | X = x, A = 1)\n",
    "  model_y0 : XGBoost model that predicts P(Y = 1 | X = x, A = 0)\n",
    "  \"\"\"\n",
    "  \n",
    "  model_a = learn_xgb(train_obs[x_idx], train_obs[a_idx])\n",
    "\n",
    "  x_idx_in = [x for j, x in enumerate(x_idx) if j not in instruments]\n",
    "  \n",
    "  a_train = train_rct[a_idx]\n",
    "  model_y1 = learn_xgb(train_rct[x_idx_in][a_train==1], train_rct[y_idx][a_train==1])\n",
    "  model_y0 = learn_xgb(train_rct[x_idx_in][a_train==0], train_rct[y_idx][a_train==0])\n",
    "  \n",
    "  a_train = train_obs[a_idx]\n",
    "  model_y1o = learn_xgb(train_obs[x_idx_in][a_train==1], train_obs[y_idx][a_train==1])\n",
    "  model_y0o = learn_xgb(train_obs[x_idx_in][a_train==0], train_obs[y_idx][a_train==0])\n",
    "\n",
    "  return model_a, model_y1, model_y0, model_y1o, model_y0o\n",
    "\n",
    "def bound_po(model_ya, model_a, x_value, x_pos, x_space, instruments, eps, test_values=None):\n",
    "   \n",
    "   \"\"\"\n",
    "   Given a model for P(Y_a = 1 | X = `x_value`) and P(A = 1 | X = `x_value`), this returns lower and upper bounds\n",
    "   on P(A = 1 | X = `x_value`, Y_a = 1). (Notice that \"a\" in {0, 1} is implicit here, given indirectly by \n",
    "   P(Y_a = 1 | X = `x_value') model). It does so by making use of X[`x_pos`] as an instrument, and assuming \n",
    "   particular constraints bounded by `eps`.\n",
    "\n",
    "   Parameters:\n",
    "   -----------\n",
    "\n",
    "   model_ya : a model that returns P(Y_1 = 1 | X = x), a xgboost tree\n",
    "   model_a  : a model that returns P(A = 1 | X = x), a xgboost tree\n",
    "   x_value  : instance of covariates where we are evaluating the quantity of interest\n",
    "   x_pos    : position in the covariate vector of the variable which is the inferred instrument\n",
    "   x_space  : an integer indicating that X[x_pos] takes values in 0, 1, 2, ..., x_space - 1\n",
    "   instruments: subset of covariates (given as positions in `x_value`) which\n",
    "              should not be used for the outcome model\n",
    "   eps      : corresponding bound to define constraints\n",
    "   test_values: for debugging purposes, if true we put here the test values of the decision variables\n",
    "                to check whether constrains hold\n",
    "\n",
    "   Returns:\n",
    "   --------\n",
    "    \n",
    "   l, u : lower and upper bounds, respectively on P(A = 1 | X = `x_value`, Y_a = 1)\n",
    "   \"\"\"\n",
    "   \n",
    "   xg = x_value[0, x_pos] # The value taken by the instrument with the respective level `x_value`.\n",
    "   x_value_in = np.delete(x_value[0, :], instruments)[None, :]\n",
    "  \n",
    "   # We will now build a small linear program with 2 * x_space decision variables. The decision variables are\n",
    "   # correspond to theta_1x := P(A = 1 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos], Y_a = 1), and\n",
    "   # theta_0x := P(A = 1 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos], Y_a = 0), where \\xpos means\n",
    "   # \"everything in range(x_space), but x_pos\". This program will have x_space equality constraints, \n",
    "   # and 4 * (x_space - 1) inequality constraints.\n",
    "\n",
    "   # Equality constraints: \n",
    "   # P(A = 1 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos], Y_a = 1) * \n",
    "   #     P(Y_a = 1 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos]) +\n",
    "   # P(A = 1 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos], Y_a = 0) * \n",
    "   #     P(Y_a = 0 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos]) = \n",
    "   # P(A = 1 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos])\n",
    "   \n",
    "   A_eq = np.zeros([x_space, 2 * x_space])\n",
    "   b_eq = np.zeros(x_space)\n",
    "   for x in range(x_space):\n",
    "     x_value_x = x_value.copy()\n",
    "     x_value_x[0, x_pos] = x\n",
    "     x_value_x_in = np.delete(x_value_x[0, :], instruments)[None, :]\n",
    "     A_eq[x, x] = model_ya.predict_proba(x_value_x_in)[0][1] \n",
    "     A_eq[x, x_space + x] = model_ya.predict_proba(x_value_x_in)[0][0]\n",
    "     b_eq[x] = model_a.predict_proba(x_value_x)[0][1]\n",
    "\n",
    "   # Inequality constraints: \n",
    "   # P(A = 1 | X[x_pos] = x_value[x_pos], X[\\x_pos] = x_value[\\x_pos], Y_a = 1) * \n",
    "   #    P(Y_a = 1 | X[x_pos] = x_value[x_pos], X[\\x_pos] = x_value[\\x_pos]) /\n",
    "   #        P(A = 1 | X[x_pos] = x_value[x_pos], X[\\x_pos] = x_value[\\x_pos])) -   \n",
    "   # P(A = 1 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos], Y_a = 1) * \n",
    "   #    P(Y_a = 1 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos]) /\n",
    "   #        P(A = 1 | X[x_pos] = x, X[\\x_pos] = x_value[\\x_pos])) <= eps\n",
    "   # \n",
    "   # Then, the same constraint, but now >= -eps. \n",
    "   #\n",
    "   # Then, a repeat of the above.\n",
    "\n",
    "   A_ineq = np.zeros([4 * (x_space - 1), 2 * x_space])\n",
    "   b_ineq = np.zeros(4 * (x_space - 1))\n",
    "   ineq_count = 0\n",
    "\n",
    "   for x in range(x_space):\n",
    "\n",
    "      if xg == x:\n",
    "        continue\n",
    "\n",
    "      x_value_x = x_value.copy()\n",
    "      x_value_x[0, x_pos] = x\n",
    "      x_value_x_in = np.delete(x_value_x[0, :], instruments)[None, :]\n",
    "      p_y0_x_value, p_y1_x_value = model_ya.predict_proba(x_value_in)[0][0], model_ya.predict_proba(x_value_in)[0][1]\n",
    "      p_y0_x_value_x, p_y1_x_value_x = model_ya.predict_proba(x_value_x_in)[0][0], model_ya.predict_proba(x_value_x_in)[0][1]\n",
    "      p_a1_x_value, p_a1_x_value_x = model_a.predict_proba(x_value)[0][1], model_a.predict_proba(x_value_x)[0][1]\n",
    "   \n",
    "      A_ineq[ineq_count, xg] = p_y1_x_value * p_a1_x_value_x\n",
    "      A_ineq[ineq_count, x] = -p_y1_x_value_x * p_a1_x_value\n",
    "      b_ineq[ineq_count] = eps * p_a1_x_value * p_a1_x_value_x\n",
    "      ineq_count = ineq_count + 1 \n",
    "   \n",
    "      A_ineq[ineq_count, xg] = -p_y1_x_value * p_a1_x_value_x\n",
    "      A_ineq[ineq_count, x] = p_y1_x_value_x * p_a1_x_value\n",
    "      b_ineq[ineq_count] = eps * p_a1_x_value * p_a1_x_value_x\n",
    "      ineq_count = ineq_count + 1 \n",
    "\n",
    "      A_ineq[ineq_count, x_space + xg] = p_y0_x_value * p_a1_x_value_x\n",
    "      A_ineq[ineq_count, x_space + x] = -p_y0_x_value_x * p_a1_x_value\n",
    "      b_ineq[ineq_count] = eps * p_a1_x_value * p_a1_x_value_x\n",
    "      ineq_count = ineq_count + 1\n",
    "\n",
    "      A_ineq[ineq_count, x_space + xg] = -p_y0_x_value * p_a1_x_value_x\n",
    "      A_ineq[ineq_count, x_space + x] = p_y0_x_value_x * p_a1_x_value\n",
    "      b_ineq[ineq_count] = eps * p_a1_x_value * p_a1_x_value_x\n",
    "      ineq_count = ineq_count + 1\n",
    "\n",
    "   # Solve program\n",
    "\n",
    "   if test_values is not None:\n",
    "     lhs = A_ineq @ test_values\n",
    "     for i in range(len(b_ineq)):\n",
    "       print(lhs[i], ' <= ', b_ineq[i], end=\"\")\n",
    "       if lhs[i] <= b_ineq[i]:\n",
    "         print(' yes')\n",
    "       else:\n",
    "         print(' NO')\n",
    "     print('Maximum equality violation = ', np.max(np.abs(A_eq @ test_values - b_eq)))\n",
    "     print('Maximum inequality gap = ', np.max(A_ineq @ test_values - b_ineq))\n",
    "     return None, None\n",
    "\n",
    "   bounds_matrix = np.zeros([2 * x_space, 2])\n",
    "   bounds_matrix[:, 1] = 1\n",
    "   bounds = [tuple(row) for row in bounds_matrix]\n",
    "   c = np.zeros(2 * x_space)\n",
    "\n",
    "   c[xg] = 1\n",
    "   result = linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ineq, b_ub=b_ineq, bounds=bounds)\n",
    "   l = result.fun\n",
    "   l_theta = result.x\n",
    "\n",
    "   c[xg] = -1\n",
    "   result = linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ineq, b_ub=b_ineq, bounds=bounds)\n",
    "   u = result.fun\n",
    "   u_theta = result.x\n",
    "   if u is not None:\n",
    "     u = -u\n",
    "\n",
    "   return l, u, l_theta, u_theta\n",
    "\n",
    "def bound_finding(x_spaces, eps, train_idx, test_idx, dat_obs, instruments,\n",
    "                  models_a, models_y1, models_y0, models_y1o, models_y0o):\n",
    "\n",
    "  \"\"\"\n",
    "  Given a model for P(Y_a = 1 | X = `x_value`) and P(A = 1 | X = `x_value`), this returns lower and upper bounds\n",
    "  on P(A = 1 | X = `x_value`, Y_a = 1). (Notice that \"a\" in {0, 1} is implicit here, given indirectly by \n",
    "  P(Y_a = 1 | X = `x_value') model). It does so by making use of X[`x_pos`] as an instrument, and assuming \n",
    "  particular constraints bounded by `eps`.\n",
    "\n",
    "  Parameters:\n",
    "  -----------\n",
    "\n",
    "  x_spaces : an array of integers indicating that X[i] takes values in 0, 1, 2, ..., x_spaces[i] - 1\n",
    "  eps      : corresponding bound to define constraints\n",
    "  train_idx : an array indicating which elements of dat_obs are to be used as training set\n",
    "  test_idx : an array indicating which elements of dat_obs are to be used as test set\n",
    "  dat_obs  : a list where dat_obs[i] is an observabtional dataset\n",
    "  instruments : a list where instruments[i] are the positions of the covariates which are instruments\n",
    "                in test set i\n",
    "  models_a  : a list wheere models_a[i] is model that returns P(A = 1 | X = x), a xgboost tree, \n",
    "              for environment i\n",
    "  models_y1 : analogous, but models_y1[i] is a model that returns P(Y_1 | X = x)\n",
    "  models_y0 : analogous, but models_y0[i] is a model that returns P(Y_0 | X = x)\n",
    "  models_y1o : analogous, but models_y1[i] is a model that returns P(Y | A = 1, X = x)\n",
    "  models_y0o : analogous, but models_y1[i] is a model that returns P(Y | A = 0, X = x)\n",
    "\n",
    "  x_value  : instance of covariates where we are evaluating the quantity of interest\n",
    "  x_pos    : position in the covariate vector of the variable which is the inferred instrument\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "    \n",
    "  lower_bounds_1, upper_bounds_1 : bounds for P(Y_1 = 1 | X = x) in each test environment\n",
    "  lower_bounds_0, upper_bounds_0 : bounds for P(Y_0 = 1 | X = x) in each test environment\n",
    "  \"\"\"\n",
    "\n",
    "  def combine_bounds(lower_bounds, upper_bounds):\n",
    "    \"\"\"\n",
    "    Combination rule for bounds extracted from training environments. Using here the simplest, most\n",
    "    conservative rule, which is likely be too conservative: it is enough to have one bad environment\n",
    "    where the selected instrument is weakly conditionally associated with treatment to \n",
    "    get trivial bounds.\n",
    "    \"\"\"\n",
    "    if len(lower_bounds) > 0:\n",
    "      l = min(lower_bounds)\n",
    "    else:\n",
    "      l = 0\n",
    "    if len(upper_bounds) > 0:\n",
    "      u = max(upper_bounds)\n",
    "    else:\n",
    "      u = 1\n",
    "    return l, u\n",
    "\n",
    "  eps = 0.01                         # Hyperparameter for bounding procedure\n",
    "  num_test = len(test_idx)           # Number of test sets\n",
    "\n",
    "  cw_lower_bounds_1 = [None] * num_test # Array that stores lower bounds for P(A = 1 | X = x, Y_1 = 1, F = idle) across test sets\n",
    "  cw_lower_bounds_0 = [None] * num_test # Array that stores lower bounds for P(A = 1 | X = x, Y_1 = 0, F = idle) across test sets\n",
    "  cw_upper_bounds_1 = [None] * num_test # Array that stores upper bounds for P(A = 1 | X = x, Y_1 = 1, F = idle) across test sets\n",
    "  cw_upper_bounds_0 = [None] * num_test # Array that stores upper bounds for P(A = 1 | X = x, Y_1 = 0, F = idle) across test sets\n",
    "  lower_bounds_1 = [None] * num_test    # Array that stores lower bounds for P(Y(A = 1) | X = x) across test sets\n",
    "  lower_bounds_0 = [None] * num_test    # Array that stores lower bounds for P(Y(A = 0) | X = x) across test sets\n",
    "  upper_bounds_1 = [None] * num_test    # Array that stores upper bounds for P(Y(A = 1) | X = x) across test sets\n",
    "  upper_bounds_0 = [None] * num_test    # Array that stores upper bounds for P(Y(A = 0) | X = x) across test sets\n",
    "\n",
    "  for i, test_i in enumerate(test_idx): # Go through each test set\n",
    "\n",
    "    n_i, n_j = dat_obs[test_i].shape[0], len(instruments[test_i])\n",
    "\n",
    "    cw_lower_bounds1_j = np.zeros([n_i, n_j]) # P(A = 1 | X = x, Y_1 = 1, F = idle) bounds stratified by instrument\n",
    "    cw_upper_bounds1_j = np.ones([n_i, n_j])\n",
    "    cw_lower_bounds0_j = np.zeros([n_i, n_j])\n",
    "    cw_upper_bounds0_j = np.ones([n_i, n_j])\n",
    "    cw_lower_bounds_1[i] = np.zeros(n_i)      # P(A = 1 | X = x, Y_1 = 1, F = idle) bounds aggregated through all instruments\n",
    "    cw_lower_bounds_0[i] = np.zeros(n_i)\n",
    "    cw_upper_bounds_1[i] = np.zeros(n_i)\n",
    "    cw_upper_bounds_0[i] = np.zeros(n_i)                           \n",
    "    lower_bounds_1[i] = np.zeros(n_i)         # P(Y(A = *) | X = x)  bounds in this test set\n",
    "    lower_bounds_0[i] = np.zeros(n_i)\n",
    "    upper_bounds_1[i] = np.zeros(n_i)\n",
    "    upper_bounds_0[i] = np.zeros(n_i)\n",
    "\n",
    "    print('Getting bounds, test set', i)\n",
    "    for m in tqdm(range(n_i)): # Go through each data point in test set i\n",
    "\n",
    "      # Extract corresponding covariate values at which we will bound causal response\n",
    "      x_value = dat_obs[test_i][x_idx].iloc[[m], :].to_numpy()\n",
    "\n",
    "      for j, x_pos in enumerate(instruments[test_i]): # For each instrument from test set \n",
    "        \n",
    "        ls1, us1 = [], []         # Lower and upper bounds of cross-world P(A = 1 | X = x_value, Y_* = 1, F = idle)\n",
    "        ls0, us0 = [], []         # Lower and upper bounds of cross-world P(A = 1 | X = x_value, Y_* = 0, F = idle)\n",
    "        x_space = x_spaces[x_pos] # Number of categories of instrument x_pos\n",
    "        \n",
    "        for v in train_idx: # For each training environment\n",
    "          if x_pos in instruments[v]: # Only look at those training sets where that variable is also a instrument\n",
    "            # Lower and upper bounds of cross-world P(A = 1 | X = x_value, Y_* = 1, F = idle) at environment v\n",
    "            l1, u1, _, _ = bound_po(models_y1[v], models_a[v], x_value, x_pos, x_space, instruments[v], eps)\n",
    "            # Lower and upper bounds of cross-world P(A = 1 | X = x_value, Y_* = 1, F = idle) at environment v\n",
    "            l0, u0, _, _ = bound_po(models_y0[v], models_a[v], x_value, x_pos, x_space, instruments[v], eps)\n",
    "            if l1 is not None: # Only add if problem is feasible (it will be infeasiable if dependence measure is not weak enough)\n",
    "              ls1.append(l1)\n",
    "              us1.append(u1)\n",
    "              ls0.append(l0)\n",
    "              us0.append(u0)\n",
    "\n",
    "        # Transfer rule: combine bounds across environments for test point m as given by j-th instrument x_pos\n",
    "        cw_lower_bounds1_j[m, j], cw_upper_bounds1_j[m, j] = combine_bounds(ls1, us1)\n",
    "        cw_lower_bounds0_j[m, j], cw_upper_bounds0_j[m, j] = combine_bounds(ls0, us0)\n",
    "      \n",
    "      # Now combine bounds across all instruments by getting maximum lower bound and minimum lower bound\n",
    "      cw_lower_bounds_1[i][m] = max(cw_lower_bounds1_j[m, :])\n",
    "      cw_upper_bounds_1[i][m] = min(cw_upper_bounds1_j[m, :])\n",
    "      cw_lower_bounds_0[i][m] = max(cw_lower_bounds0_j[m, :])\n",
    "      cw_upper_bounds_0[i][m] = min(cw_upper_bounds0_j[m, :])\n",
    "\n",
    "      # Final step: now that we have bounds on cross-world P(A = 1 | X = x_value, Y_* = 1, F = idle),\n",
    "      # propagate these to get bounds on P(Y(A = 0) = 1 | X = x_value) and P(Y(A = 1) = 1 | X = x_value)\n",
    "      x_value_sel = np.delete(x_value[0, :], instruments[test_i]).reshape(1, len(x_idx) - len(instruments[test_i]))\n",
    "      p_y_1 = models_y1o[i].predict_proba(x_value_sel)[0][1] # P(Y = 1 | A = 1, X = x_value)\n",
    "      p_a_1 = models_a[i].predict_proba(x_value)[0][1]   # P(A = 1 | X = x_value)\n",
    "      p_y_0 = models_y0o[i].predict_proba(x_value_sel)[0][1] # P(Y = 1 | A = 0, X = x_value)\n",
    "      p_a_0 = models_a[i].predict_proba(x_value)[0][0]   # P(A = 0 | X = x_value)\n",
    "      lower_bounds_1[i][m] = p_y_1 * p_a_1 / cw_upper_bounds_1[i][m]\n",
    "      lower_bounds_0[i][m] = p_y_0 * p_a_0 / cw_upper_bounds_0[i][m]\n",
    "      if cw_lower_bounds_1[i][m] > 0:\n",
    "        upper_bounds_1[i][m] = p_y_1 * p_a_1 / cw_lower_bounds_1[i][m]\n",
    "        if upper_bounds_1[i][m] > 1:\n",
    "          upper_bounds_1[i][m] = 1\n",
    "      else:\n",
    "        upper_bounds_1[i][m] = 1\n",
    "      if cw_lower_bounds_0[i][m] > 0:\n",
    "        upper_bounds_0[i][m] = p_y_0 * p_a_0 / cw_lower_bounds_0[i][m]\n",
    "        if upper_bounds_0[i][m] > 1:\n",
    "          upper_bounds_0[i][m] = 1\n",
    "      else:\n",
    "        upper_bounds_0[i][m] = 1\n",
    "\n",
    "  return lower_bounds_1, upper_bounds_1, lower_bounds_0, upper_bounds_0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773bcc9",
   "metadata": {},
   "source": [
    "-------\n",
    "## Example of a Run\n",
    "\n",
    "The following is a full end-to-end run of the algorithm, making use of a fully synthetic simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0834d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating models and data... Done!\n"
     ]
    }
   ],
   "source": [
    "# Simulation study: generate synthetic models and datasets\n",
    "# This uses a very bare-bones simulator that should be substituted.\n",
    "#\n",
    "# In all that follows:\n",
    "#  - we will use A to denote binary treatment variable\n",
    "#  - we will use X to denote (categorical) covariates\n",
    "#  - we will use Y to denote binary outcome variables\n",
    "#\n",
    "# Notice that in the manuscript, we used X to denote treatments and Z to denote covariates.\n",
    "#\n",
    "# The main data structure used here to store data are Pandas dataframes, where categorical\n",
    "# variables are represented explicitly as categories. No one-hot encoding is used, it's one\n",
    "# column per categorical variable.\n",
    "\n",
    "k = 10         # Number of datasets\n",
    "n_obs = 200    # Sample size for observational datasets\n",
    "n_rct = 200    # Sample size for RCTs\n",
    "num_x = 20     # Number of covariates\n",
    "p_sparse = 0.2 # Probability of dropping a covariate in the outcome model\n",
    "\n",
    "# Simulate true models (using a very poorly tested simulator as a placeholder only).\n",
    "# Simulated models are all binary with some edges from covariates to outcome removed.\n",
    "# The conditional distribution of each variable given parents is a logistic regression model.\n",
    "#\n",
    "# Notice though that we define two separate logistic regression models for the outcome stratified by treatment.\n",
    "# That is, P(Y(A = 1) | X = x) and P(Y(A = 0) | X = x) are two distinctive models. So P(Y | A = a, X = x) is \n",
    "# non-linear on A.\n",
    " \n",
    "print('Simulating models and data... ', end=\"\")\n",
    "true_models, dat_obs, dat_rct, true_cates = simulate_plain_data_collection(k, n_obs, n_rct, num_x, p_sparse)\n",
    "train_idx = range(k - 2)  # Environments for training\n",
    "test_idx = [k - 2, k - 1] # Environments for testing\n",
    "print('Done!')\n",
    "\n",
    "# Here, x_idx refers to a list of strings denoting column names for covariates, a_idx is a string denoting\n",
    "# column name for treatment, y_idx is a string denoting the column name for the outcome\n",
    "x_idx, a_idx, y_idx = true_models[0].x_idx, true_models[0].a_idx, true_models[0].y_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ef9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering instruments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 70374.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning predictive models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# This block discover instruments in each training set. It is currently working very poorly, based\n",
    "# on a vary bare-bones sparse logistic regression algorithm\n",
    "\n",
    "print('Discovering instruments...')\n",
    "instruments = [None] * k\n",
    "for i in tqdm(range(k)):\n",
    "  if true_models is not None:\n",
    "    instruments[i] = np.where(true_models[0].coeff_y1 == 0)[0] - 1\n",
    "  else:\n",
    "    # This is not working well at all, which is the reason currently we are relying on true_models\n",
    "    instruments[i] = search_adjustment(dat_obs[i], x_idx, a_idx, y_idx)\n",
    "    \n",
    "print('Learning predictive models...') # Currently, this is slow (xgboost with cross-validation). Maybe someone can improve it?\n",
    "models_a, models_y1, models_y0, models_y1o, models_y0o = [None] * k, [None] * k, [None] * k, [None] * k, [None] * k\n",
    "for i in tqdm(range(k)): \n",
    "  # models_a[i] is the propensity score for each environment (training and test)\n",
    "  # models_y1[i] is a model for P(Y(A = 1) = y | X = x) directly from RCT data (assumes RCT data available to all datasets)\n",
    "  # models_y0[i] is a model for P(Y(A = 0) = y | X = x) directly from RCT data\n",
    "  # models_y1o[i] is a model for P(Y = y | A = 1, X = x) from observational data\n",
    "  # models_y0o[i] is a model for P(Y = y | A = 0, X = x) from observational data\n",
    "  models_a[i], models_y1[i], models_y0[i], models_y1o[i], models_y0o[i] = \\\n",
    "    get_predictive_model(dat_obs[i], dat_rct[i], x_idx, a_idx, y_idx, instruments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bda1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This blocks return lower and upper bounds of P(Y_1 | X = x) and P(Y_0 | X = x) for \n",
    "# all test sets\n",
    "\n",
    "eps = 0.01 # Hyperparameter regulating bounding procedure\n",
    "x_spaces = true_models[0].x_space  # Array that stores number of categories for each covariate\n",
    "\n",
    "lower_bounds_1, upper_bounds_1, lower_bounds_0, upper_bounds_0 = \\\n",
    "  bound_finding(x_spaces, eps, train_idx, test_idx, dat_obs, instruments,\n",
    "                models_a, models_y1, models_y0, models_y1o, models_y0o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebdf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison:\n",
    "#\n",
    "# Let's pretend that regime \"off\" if P(Y | A = 1, X = x) and that regime \"on\" is P(Y(A = 1) | X = x)\n",
    "# We will then compare the true P(Y(A = 1) = 1 | X = x) and the bounds, plus we will compare the \"off\" regime and \"on\"\n",
    "# regime, indicating when the lower bound of the \"on\" regime is higher that the corresponding outcome on \n",
    "# the \"off\" regime.\n",
    "\n",
    "num_test = len(test_idx)           # Number of test sets\n",
    "\n",
    "p_off = [None] * num_test\n",
    "p_gold = [None] * num_test \n",
    "hit_ratio = np.zeros(num_test)\n",
    "informative_ratio = np.zeros(num_test)\n",
    "\n",
    "for i, test_i in enumerate(test_idx): # Go through each test set\n",
    "  x_idx_sel = [x for w, x in enumerate(x_idx) if w not in instruments[test_idx[0]]]\n",
    "  p_off[i] = models_y1o[i].predict_proba(dat_obs[test_i][x_idx_sel])[:, 1]\n",
    "  p_gold[i] = models_y1[i].predict_proba(dat_obs[test_i][x_idx_sel])[:, 1]\n",
    "  hit_ratio[i] = np.mean((p_gold[i] <= upper_bounds_1[i]) * (p_gold[i] >= lower_bounds_1[i]))\n",
    "  informative_ratio[i] = np.mean(p_off[i] <= lower_bounds_1[i])\n",
    "  print('Interval coverage:', hit_ratio[i], 'for test set', i)\n",
    "  print('Informativeness ratio:', informative_ratio[i], 'for test set', i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7db008",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((p_gold[i] <= upper_bounds_1[i]) * (p_gold[i] >= lower_bounds_1[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_gold[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f613e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

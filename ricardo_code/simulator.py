################################################################################################
################################################################################################
################################################################################################
############ Library loading

import numpy as np
import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
import copy

from dataclasses import dataclass
from multipledispatch import dispatch
from sklearn.cluster import KMeans
from scipy.stats import invwishart, spearmanr
from sklearn.ensemble import IsolationForest
from sklearn.metrics import confusion_matrix
from sklearn.decomposition import FastICA
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.model_selection import GridSearchCV

################################################################################################
################################################################################################
################################################################################################
############ Utilities in general

# Converts columns in a DataFrame to categorical where the number of unique values
# is less than or equal to `cat_threshold`.
def dataframe_categorize(df, cat_threshold, full_cat=False):
  for column in df.columns:
    if df[column].nunique() <= cat_threshold:
      df[column] = df[column].astype(int).astype('category')
    elif full_cat:
      x = np.asarray(df[column]).reshape(-1, 1)
      km = KMeans(n_clusters=cat_threshold, n_init=10)
      df[column] = km.fit_predict(x)
      df[column] = df[column].astype(int).astype('category')

# The following was generated by Copilot
def cov_to_corr(cov_matrix):
  """
  Converts a covariance matrix to a correlation matrix.

  Parameters:
      cov_matrix (numpy.ndarray): Covariance matrix.

  Returns:
      numpy.ndarray: Correlation matrix.
  """
  std_dev = np.sqrt(np.diag(cov_matrix))
  outer_std_dev = np.outer(std_dev, std_dev)
  corr_matrix = cov_matrix / outer_std_dev
  corr_matrix[np.diag_indices_from(corr_matrix)] = 1    
  return corr_matrix

# Logistic function
def logistic(x):
    return 1 / (1 + np.exp(-x))

# Learn a XGBoost (binary) classifier or regression function.
# 

def learn_xgb(input_dat, output_dat):

  param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [3, 4, 5],
    'learning_rate': [0.01, 0.1, 0.2]
  }

  if isinstance(output_dat.dtype, pd.CategoricalDtype):
    bst = xgb.XGBClassifier(objective='binary:logistic', enable_categorical=True)
  else:
    bst = xgb.XGBRegressor(objective='reg:squarederror', enable_categorical=True)

  grid_search = GridSearchCV(estimator=bst, param_grid=param_grid, cv=3, scoring='accuracy', verbose=0)
  grid_search.fit(input_dat, output_dat)
  bst = grid_search.best_estimator_

  return bst

# Learn a (log) linear (binary) classifier or regression function.
# 

def learn_linear(input_dat, output_dat):

  if isinstance(output_dat.dtype, pd.CategoricalDtype):
    model = LogisticRegression()
  else:
    model = LinearRegression()
  model.fit(input_dat, output_dat)

  return model

# Predict expected value of output of a XGBoost model `bst` based on
# input `x_val` and whether output is categorical (`y_dtype`). Only binary
# and continuous outcomes are allowed.

def predict_xgb(bst, x_eval, y_dtype):
  if isinstance(y_dtype, pd.CategoricalDtype):
    return bst.predict_proba(x_eval)[:, 1]
  return bst.predict(x_eval)

# Print a confusion matrix

def print_confusion_matrix(y_true, y_pred, labels=None):
  cm = confusion_matrix(y_true, y_pred, labels=labels)
  cm_df = pd.DataFrame(cm, index=labels, columns=labels)
  print('Confusion Matrix:')
  print(cm_df)

# Samples from a conditional multivariate Gaussian distribution

def conditional_multivariate_gaussian(mu, Sigma, indices_given, values_given, n):
  """
  Samples from a conditional multivariate Gaussian distribution.
    
  Parameters:
  mu (ndarray): Mean vector of the full distribution.
  Sigma (ndarray): Covariance matrix of the full distribution.
  indices_given (list): Indices of variables that are conditioned on.
  values_given (ndarray): Values of the conditioned variables.
  n: number of samples from each value given
  
  Returns:
  ndarray: A sample from the conditional distribution.
  """
  m = values_given.shape[0]
  indices_free = [i for i in range(len(mu)) if i not in indices_given]

  mu_given = mu[indices_given]
  mu_free = mu[indices_free]
  Sigma_gg = Sigma[np.ix_(indices_given, indices_given)]
  Sigma_fg = Sigma[np.ix_(indices_free, indices_given)]
  Sigma_ff = Sigma[np.ix_(indices_free, indices_free)]

  # Compute conditional mean and covariance
  Sigma_gg_inv = np.linalg.inv(Sigma_gg)
  mu_cond = mu_free + Sigma_fg @ Sigma_gg_inv @ (values_given - mu_given)
  Sigma_cond = Sigma_ff - Sigma_fg @ Sigma_gg_inv @ Sigma_fg.T

  # Sample from the conditional distribution
  samples = [None] * m
  for i in range(m):
    samples[i] = np.random.multivariate_normal(mu_cond, Sigma_cond, size=n)
  return samples


################################################################################################
################################################################################################
################################################################################################
############ Parameter Learning, Simulation and Ground Truth

""" 
The following piece of code fits data to a classic causal model that is fully defined by a triplet: a (binary) treatment variable $A$, some (binary or continuous) outcome $Y$, given pre-treatment covariates $X$. Conditional ignorability is assumed.

There is also code for simulating from a fitted model, and a method to generate CATEs from a given model. The latter means that if a model is the ground truth one, the corresponding CATEs are ground truth. 

Given a model, we also have a method (`confounder_column_keeper`) to select a subset of columns to keep, while hiding others, in order to generate data from a model which violates conditional ignorability.

We use `pandas.DataFrame` as our main data structure to facilitate the use of categorical data in `xgboost`.
 """

# Class BackdoorModel:
#
# This stores structural information for models composed on synthetic models, which are based on(correlated) standard Gaussians
# as covariates, with treatment and outcome being binary variables generated by a logistic model.

@dataclass
class BackdoorModel:
  x_idx: str                   # Column names of covariates
  a_idx: str                   # Column name of treatment
  y_idx: str                   # Column name of outcome

# Class XGBoostBackdoorModel:
#
# This stores information for models fit to data, based on XGBoost for treatment and outcome,
# and a plain empirical distribution for the covariates. 

@dataclass
class XGBoostBackdoorModel(BackdoorModel):
  bst_propensity: xgb.XGBModel # Propensity score model
  bst_outcome: xgb.XGBModel    # Outcome model
  std_outcome: float           # Standard deviation of outcome regression, if applicable
  empirical_df: pd.DataFrame   # Empirical distribution for generating covariates     

# Parameter learning of basic causal model with triplet (treatment, outcome, pre-treatment covariates).
# 
# This method adopts no model for covariates (we will use the empirical distribution). It assumes
# a binary treatment, and either a binary or continuous outcome. It uses XGBoost for conditional 
# means and, in case out a continuous outcome, a homescedastic Gaussian error model.
#
# Inputs:
#
# - `train`, a DataFrame with the data 
# - `x_idx`, names covariate columns
# - `a_idx`, name of treatment column
# - `y_idx`, name of outcome column
#
# This returns the model for propensities, the model for the outcome, and if outcome
# is continuous, the standard deviation of the error term, all wrapped up in
# a `XGBoostBackdoorModel` object.
#
# Call this method if you want to fit a generative model. If all you wish is a CATE
# estimator, use something else like estimate_cate_tlearner.

def learn_backdoor_model(train, x_idx, a_idx, y_idx):
  
  x_train, a_train, y_train = train[x_idx], train[a_idx], train[y_idx]
  ax_train = pd.concat([a_train, x_train], axis=1)

  bst_propensity = learn_xgb(x_train, a_train)
  bst_outcome = learn_xgb(ax_train, y_train)

  if isinstance(y_train.dtype, pd.CategoricalDtype):
    std_outcome = None
  else:
    std_outcome = np.std(bst_outcome.predict(ax_train))

  return XGBoostBackdoorModel(bst_propensity=bst_propensity, bst_outcome=bst_outcome, std_outcome=std_outcome,
                              x_idx=x_idx, a_idx=a_idx, y_idx=y_idx, empirical_df=train)

# Simulation from a fitted model.
#
# Basic simulation out of a causal model based on a XGBoost fit. Covariates are generated
# from a boring model-free bootstrap sample out of `model.empirical_pd`.
#
# The 'controlled' version emulates a RCT where treatment A is sampled from a Bernoulli(0.5)
#
# Inputs:
#
# - `n` is the number of samples
# - `model` is a fitted model of the type `XBoostBackdoorModel`

@dispatch(int, XGBoostBackdoorModel)
def simulate_from_backdoor_model(n, model):
  row_choices = np.random.choice(range(model.empirical_df.shape[0]), size=n)
  df_sample = model.empirical_df.iloc[row_choices, :].reset_index(drop=True)
  propensities = model.bst_propensity.predict_proba(df_sample[model.x_idx])[:, 1]
  df_sample[model.a_idx] = np.random.binomial(n=1, p=propensities)
  df_sample[model.a_idx] = df_sample[model.a_idx].astype('category') # Somehow, this is necessary

  e_y = predict_xgb(model.bst_outcome, df_sample[[*[model.a_idx], *model.x_idx]], model.empirical_df[model.y_idx].dtype)
  if model.std_outcome == None:
    df_sample[model.y_idx] = np.random.binomial(n=1, p=e_y)
    df_sample[model.y_idx] = df_sample[model.y_idx].astype('category') # Somehow, this is necessary
  else:
    df_sample[model.y_idx] = e_y + np.random.normal(loc=0, scale=model.std_outcome, size=n)

  return df_sample

@dispatch(int, XGBoostBackdoorModel)
def simulate_from_controlled_backdoor_model(n, model):
  row_choices = np.random.choice(range(model.empirical_df.shape[0]), size=n)
  df_sample = model.empirical_df.iloc[row_choices, :].reset_index(drop=True)
  df_sample[model.a_idx] = np.random.binomial(n=1, p=0.5 * np.ones(n))
  df_sample[model.a_idx] = df_sample[model.a_idx].astype('category') # Somehow, this is necessary

  e_y = predict_xgb(model.bst_outcome, df_sample[[*[model.a_idx], *model.x_idx]], model.empirical_df[model.y_idx].dtype)
  if model.std_outcome == None:
    df_sample[model.y_idx] = np.random.binomial(n=1, p=e_y)
    df_sample[model.y_idx] = df_sample[model.y_idx].astype('category') # Somehow, this is necessary
  else:
    df_sample[model.y_idx] = e_y + np.random.normal(loc=0, scale=model.std_outcome, size=n)

  return df_sample

# Selection of observable columns.
#
# This decides which covariates to keep so that everything else will count as unmeasured 
# confounding. It uses XGBoost's built-in measure of variable importance to decide 
# which variables to be thrown out.
#
# Inputs:
#
# - `keep_k`: how many of the covariates to keep as observable.
# - `model`: the corresponding `XGBoostBackdoorModel` model.

@dispatch(int, XGBoostBackdoorModel)
def confounder_column_keeper(keep_k, model):
  keep_x = np.argsort(model.bst_propensity.feature_importances_)[-keep_k:]
  x_select = [model.x_idx[i] for i in keep_x]
  return x_select

# Compute CATE from given model.
#
# Inputs:
#
# - `df_sample` is the table of covariates we evaluate at
# - `model` is the corresponding `XGBoostBackdoorModel` fitted model.

@dispatch(pd.DataFrame, XGBoostBackdoorModel)
def get_cate(df_sample, model):
  
  n = df_sample.shape[0]
  a_value = np.zeros([n, 1])
  cate = np.zeros(n, dtype=np.float32)

  for i in [0, 1]:
    a = pd.DataFrame(a_value, columns=[model.a_idx]).astype(int).astype('category')
    ax_sample = pd.concat([a, df_sample[model.x_idx]], axis=1)
    e_y = predict_xgb(model.bst_outcome, ax_sample, df_sample[model.y_idx].dtype)
    cate = cate + (2 * i - 1) * e_y
    a_value = a_value + 1
  
  return cate

# The following gets a Monte Carlo estimation of CATE with respect to a model
# where only a subset of the covariates is conditioned on.
#
# It does it by:
#
# 1. Generating a large Monte Carlo sample (as given by `num_simulations`)
#    where we first "delete the edges" from X to A and do uniform sampling
#    of A.
# 2. Fit a XGBoost model for the outcome model only on the columns
#    `x_select` that have been previously chosen. This is the Monte Carlo
#    bit, as the model is an approximation to the (assumed to be know)
#    implied marginal CATE model of `model` with respect to `x_select`,
#    for which we don't have an analytical solution.
# 3. Apply the XGBoost model to the units in `df_sample` with respect o
#    `x_select`

def get_montecarlo_cate(data_eval, x_select, num_simulations, model, use_logistic=False):

  x_eval = data_eval[x_select]
  cate_eval = np.zeros(x_eval.shape[0])
  monte_carlo_sample = simulate_from_controlled_backdoor_model(num_simulations, model)

  for i in [0, 1]:
    sel_train = monte_carlo_sample[model.a_idx] == i 
    x_train = monte_carlo_sample[x_select][sel_train]
    y_train = monte_carlo_sample[model.y_idx][sel_train]

    if use_logistic:
      lgt_montecarlo = LogisticRegression()
      lgt_montecarlo.fit(x_train, y_train)
      e_y = lgt_montecarlo.predict_proba(x_eval)[:, 1]
    else:
      bst_montecarlo = learn_xgb(x_train, y_train) 
      e_y = predict_xgb(bst_montecarlo, x_eval, y_train.dtype)
    cate_eval = cate_eval + (2 * i - 1) * e_y
      
  return cate_eval

# Estimate in-sample cate based on data that is assumed to be conditionally ignorable
# (RCTs/backdoor) using a T-Learner based on XGBoost and only covariate columns `x_select`.

def estimate_cate_tlearner(data, x_select, model):
  
  x_data = data[x_select]
  cate_hat = np.zeros(data.shape[0])
  bst_outcome = [None, None]

  for i in [0, 1]:
    sel_train = data[model.a_idx] == i 
    x_train = data[x_select][sel_train]
    y_train = data[model.y_idx][sel_train]

    bst_outcome[i] = learn_xgb(x_train, y_train)
    e_y = predict_xgb(bst_outcome[i], x_data, y_train.dtype)
    cate_hat = cate_hat + (2 * i - 1) * e_y
      
  return cate_hat, bst_outcome

# Class BinarySyntheticBackdoorModel:
#
# The following class and mehods are about the same as above, but focusing on purely synthetic
# models defined on binary variables. The propensity score is given by a log-linear model,
# as well as each potential outcome, but within each treatment level the covariate effects are
# also log-linear.

@dataclass
class BinarySyntheticBackdoorModel:
  x_idx: str                   # Column names of covariates
  a_idx: str                   # Column name of treatment
  y_idx: str                   # Column name of outcome
  prob_x: np.ndarray           # Probability of heads for each covariate
  coeff_a: np.ndarray          # Logistic coefficients for treatment
  coeff_y0: np.ndarray         # Logistic coefficients for outcome at A = 0
  coeff_y1: np.ndarray         # Logistic coefficients for outcome at A = 0
  x_space: np.ndarray          # Number of values for each categorical variable (2, by definition)

def simulate_binary_synthetic_backdoor_parameters(num_x, sparse_pattern=None):
  
  prob_x = np.random.uniform(size=num_x)
  coeff_a = np.random.normal(size=num_x + 1) / num_x
  coeff_y0 = np.random.normal(size=num_x + 1) / num_x
  coeff_y1 = np.random.normal(size=num_x + 1) / num_x
  if sparse_pattern is not None:
    sparse_pattern_x = np.insert(sparse_pattern, 0, 0.)
    coeff_y0[sparse_pattern_x==1] = 0
    coeff_y1[sparse_pattern_x==1] = 0

  a_idx = 'A'
  x_idx = ['X' + str(i) for i in range(num_x)]
  y_idx = 'Y'
  x_space = np.ones(num_x, dtype=np.int_) * 2

  return BinarySyntheticBackdoorModel(prob_x=prob_x, coeff_a=coeff_a, coeff_y0=coeff_y0, coeff_y1=coeff_y1,
                                      a_idx=a_idx, x_idx=x_idx, y_idx=y_idx, x_space=x_space) 

@dispatch(int, BinarySyntheticBackdoorModel)
def simulate_from_backdoor_model(n, model):
  
  num_x = len(model.x_idx)
  x = np.zeros([n, num_x], dtype=np.int_)
  for i in range(num_x):
    x[:, i] = np.random.binomial(n=1, p=model.prob_x[i], size=n)

  a_prob = logistic(np.concatenate((np.ones([n, 1]), x), axis=1)@model.coeff_a)
  a = np.array(np.random.binomial(n=1, p=a_prob, size=n)).reshape(n, 1)

  y_prob_0 = logistic(np.concatenate((np.ones([n, 1]), x), axis=1)@model.coeff_y0)
  y_0 = np.array(np.random.binomial(n=1, p=y_prob_0, size=n)).reshape(n, 1)
  y_prob_1 = logistic(np.concatenate((np.ones([n, 1]), x), axis=1)@model.coeff_y1)
  y_1 = np.array(np.random.binomial(n=1, p=y_prob_1, size=n)).reshape(n, 1)
  y = y_0
  y[a==1] = y_1[a==1]

  dat = pd.DataFrame(np.concatenate([a, x, y], axis=1), 
                     columns=[model.a_idx] + model.x_idx + [model.y_idx])
  dataframe_categorize(dat, 2)
  
  return dat

@dispatch(int, BinarySyntheticBackdoorModel)
def simulate_from_controlled_backdoor_model(n, model):
  num_x = len(model.x_idx)
  controlled_model = copy.deepcopy(model)
  controlled_model.coeff_a = np.zeros(num_x + 1) # Erase "edges" from X to A
  dat = simulate_from_backdoor_model(n, controlled_model) 
  return dat

@dispatch(pd.DataFrame, BinarySyntheticBackdoorModel)
def get_cate(eval_data, model):
  n = eval_data.shape[0]
  x = eval_data[model.x_idx].to_numpy().astype(float)
  y_prob_0 = logistic(np.concatenate((np.ones([n, 1]), x), axis=1)@model.coeff_y0)
  y_prob_1 = logistic(np.concatenate((np.ones([n, 1]), x), axis=1)@model.coeff_y1)
  cate = y_prob_1 - y_prob_0
  return cate
